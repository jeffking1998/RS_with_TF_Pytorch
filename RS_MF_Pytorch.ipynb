{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RS_MF_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwTdNFMcvUVZyMeIZX8QT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffking1998/pytorch_RS_D2L/blob/main/RS_MF_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Load Dataset\n",
        "\n",
        "* d2l \n",
        "The package of D2L book helps to implement the model easily. "
      ],
      "metadata": {
        "id": "h99TaWwWt4Qw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIQHEeqEtl2r",
        "outputId": "84e2b594-a852-4e0d-cc41-960550ba4eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 26 10:14:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already satisfied: d2l in /usr/local/lib/python3.7/dist-packages (0.17.5)\n",
            "Requirement already satisfied: numpy==1.21.5 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.21.5)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.7/dist-packages (from d2l) (2.25.1)\n",
            "Requirement already satisfied: matplotlib==3.5.1 in /usr/local/lib/python3.7/dist-packages (from d2l) (3.5.1)\n",
            "Requirement already satisfied: pandas==1.2.4 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.2.4)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (7.7.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (4.33.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (3.0.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (7.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->d2l) (2022.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.1->d2l) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.15.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (2.15.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (3.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "## show GPU's status\n",
        "!nvidia-smi\n",
        "\n",
        "## install compulsory modules\n",
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## import required packages \n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from d2l import torch as d2l ## ??"
      ],
      "metadata": {
        "id": "6IgBsVput0dA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download movielens-100k\n",
        "\n",
        "##save\n",
        "d2l.DATA_HUB['ml-100k'] = (\n",
        "    'https://files.grouplens.org/datasets/movielens/ml-100k.zip',\n",
        "    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')\n",
        "\n",
        "##save\n",
        "def read_data_ml100k():\n",
        "    data_dir = d2l.download_extract('ml-100k')\n",
        "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "    data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\n",
        "                       engine='python')\n",
        "    num_users = data.user_id.unique().shape[0]\n",
        "    num_items = data.item_id.unique().shape[0]\n",
        "    return data, num_users, num_items"
      ],
      "metadata": {
        "id": "WGg3icYruQun"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Statistics & Visuaization of the MovieLens-100K\n",
        "\n",
        "data, num_users, num_items = read_data_ml100k()\n",
        "sparsity = 1 - len(data) / (num_users * num_items)\n",
        "print(f'number of users: {num_users}, number of items: {num_items}')\n",
        "print(f'matrix sparsity: {sparsity:f}')\n",
        "print(data.head(5))\n",
        "\n",
        "\n",
        "d2l.plt.hist(data['rating'], bins=5, ec='black')\n",
        "d2l.plt.xlabel('Rating')\n",
        "d2l.plt.ylabel('Count')\n",
        "d2l.plt.title('Distribution of Ratings in MovieLens 100K')\n",
        "d2l.plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "tEo2V8Y_uVO2",
        "outputId": "0fd093bf-7159-4634-d343-7b2f2abdc6e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of users: 943, number of items: 1682\n",
            "matrix sparsity: 0.936953\n",
            "   user_id  item_id  rating  timestamp\n",
            "0      196      242       3  881250949\n",
            "1      186      302       3  891717742\n",
            "2       22      377       1  878887116\n",
            "3      244       51       2  880606923\n",
            "4      166      346       1  886397596\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split and Load Data into Tensor\n",
        "\n",
        "def split_data_ml100k(data, num_users, num_items,\n",
        "                      split_mode='random', test_ratio=0.1):\n",
        "    \"\"\"Split the dataset in random mode or seq-aware mode.\"\"\"\n",
        "    if split_mode == 'seq-aware':\n",
        "        train_items, test_items, train_list = {}, {}, []\n",
        "        for line in data.itertuples():\n",
        "            u, i, rating, time = line[1], line[2], line[3], line[4]\n",
        "            train_items.setdefault(u, []).append((u, i, rating, time))\n",
        "            if u not in test_items or test_items[u][-1] < time:\n",
        "                test_items[u] = (i, rating, time)\n",
        "        for u in range(1, num_users + 1):\n",
        "            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))\n",
        "        test_data = [(key, *value) for key, value in test_items.items()]\n",
        "        train_data = [item for item in train_list if item not in test_data]\n",
        "        train_data = pd.DataFrame(train_data)\n",
        "        test_data = pd.DataFrame(test_data)\n",
        "    else:\n",
        "        mask = [True if x == 1 else False for x in torch.rand(\n",
        "            (len(data))) < 1 - test_ratio]\n",
        "        neg_mask = [not x for x in mask]\n",
        "        train_data, test_data = data[mask], data[neg_mask]\n",
        "    return train_data, test_data\n",
        "\n",
        "def load_data_ml100k(data, num_users, num_items, feedback='explicit'):\n",
        "    users, items, scores = [], [], []\n",
        "    inter = torch.zeros((num_items, num_users)) if feedback == 'explicit' else {}\n",
        "    for line in data.itertuples():\n",
        "        user_index, item_index = int(line[1] - 1), int(line[2] - 1)            # start from 0\n",
        "        score = int(line[3]) if feedback == 'explicit' else 1\n",
        "        users.append(user_index)\n",
        "        items.append(item_index)\n",
        "        scores.append(score)\n",
        "        if feedback == 'implicit':\n",
        "            inter.setdefault(user_index, []).append(item_index)\n",
        "        else:\n",
        "            inter[item_index, user_index] = score\n",
        "    return users, items, scores, inter\n",
        "\n",
        "\n",
        "def split_and_load_ml100k(split_mode='seq-aware', feedback='explicit',\n",
        "                          test_ratio=0.1, batch_size=256):\n",
        "    data, num_users, num_items = read_data_ml100k()\n",
        "    train_data, test_data = split_data_ml100k(\n",
        "        data, num_users, num_items, split_mode, test_ratio)\n",
        "    train_u, train_i, train_r, _ = load_data_ml100k(\n",
        "        train_data, num_users, num_items, feedback)\n",
        "    test_u, test_i, test_r, _ = load_data_ml100k(\n",
        "        test_data, num_users, num_items, feedback)\n",
        "\n",
        "    class ML100KDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, users, items, ratings):\n",
        "            assert len(users) == len(items) == len(ratings)\n",
        "            self.users = users\n",
        "            self.items = items\n",
        "            self.ratings = ratings\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return (self.users[index], self.items[index],\n",
        "                    self.ratings[index])\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.users)\n",
        "\n",
        "    train_set = ML100KDataset(train_u, train_i, train_r)\n",
        "    test_set = ML100KDataset(test_u, test_i, test_r)\n",
        "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                                      drop_last=True)\n",
        "    test_iter = torch.utils.data.DataLoader(test_set, batch_size)\n",
        "    return num_users, num_items, train_iter, test_iter"
      ],
      "metadata": {
        "id": "UzFoM_FEvSu0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x6QnoF-m5tEg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model "
      ],
      "metadata": {
        "id": "6p3YlsCTv4cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages \n",
        "\n",
        "import torch\n",
        "from torch import nn\n"
      ],
      "metadata": {
        "id": "R8AaktYbv338"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # RMSE loss based on https://discuss.pytorch.org/t/rmse-loss-function/16540/4\n",
        "# class RMSELoss(nn.Module):\n",
        "#     def __init__(self, eps=1e-6):\n",
        "#         super().__init__()\n",
        "#         self.mse = nn.MSELoss()\n",
        "#         self.eps = eps\n",
        "        \n",
        "#     def forward(self, yhat, y):\n",
        "#         loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
        "#         return loss \n",
        "\n",
        "# # convert tuple to tensor based on https://discuss.pytorch.org/t/convert-a-tuple-into-tensor/82964/3\n",
        "# def tuple_to_tensor(tuple_tensors):\n",
        "#     return torch.stack(tuple_tensors, dim=0)\n"
      ],
      "metadata": {
        "id": "R8Qma52W561r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, num_factors, num_users, num_items, **kwargs):\n",
        "        super(MF, self).__init__(**kwargs)\n",
        "        self.P = nn.Embedding(num_embeddings=num_users, embedding_dim=num_factors)\n",
        "        self.Q = nn.Embedding(num_embeddings=num_items, embedding_dim=num_factors)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "\n",
        "    def forward(self, user_id, item_id):\n",
        "        P_u = self.P(user_id)\n",
        "        Q_i = self.Q(item_id)\n",
        "        b_u = self.user_bias(user_id)\n",
        "        b_i = self.item_bias(item_id)\n",
        "        # outputs = (P_u * Q_i).sum(axis=0) + torch.squeeze(b_u) + torch.squeeze(b_i)\n",
        "        outputs = P_u @ Q_i + b_u + b_i\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "gY2zCiXiwOMD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters"
      ],
      "metadata": {
        "id": "oikV9GmswgQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# devices = 'cuda:0'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "learning_rate = 0.002#1e-3\n",
        "batch_size = 512\n",
        "epochs = 50\n",
        "test_ratio = 0.1\n",
        "wd = 1e-5 \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o90sc7iJwfTG",
        "outputId": "abdec175-92fe-42e1-ffe0-ccfd892ab66a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SET loss_fun & optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "sslBDQRKxduX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "70KFv_aHxjMp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3pImecNoxE3f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Steps\n",
        "* Functions: Train & Test  "
      ],
      "metadata": {
        "id": "FpLql3c8yC2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    l = 0 \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    for batch, values in enumerate(dataloader):\n",
        "        # # Compute prediction and loss\n",
        "        # batch = batch.to(device)\n",
        "        # values = values.to(device)\n",
        "        u_list, i_list, r_list = values \n",
        "        u_list, i_list, r_list = u_list.to(device), i_list.to(device), r_list.to(device)\n",
        "        losses = []\n",
        "        for u,i,r in zip(u_list, i_list, r_list):\n",
        "            pred = model(u, i)\n",
        "            # print(torch.squeeze(pred).shape)\n",
        "            # print(r.float().shape)\n",
        "            loss = loss_fn(torch.squeeze(pred), r.float()) #loss_fn(pred, r) \n",
        "            losses.append(loss)\n",
        "            \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        [loss.backward() for loss in losses]\n",
        "\n",
        "        l += np.sqrt(np.mean([loss.item() for loss in losses]))\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch+1) % 10 == 0:\n",
        "            print(\"loss: {:.2f}\".format(l / (batch+1)))"
      ],
      "metadata": {
        "id": "jXNDD39nyB_d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = []\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for values in dataloader:\n",
        "            losses = []\n",
        "            u_list, i_list, r_list = values\n",
        "            u_list, i_list, r_list = u_list.to(device), i_list.to(device), r_list.to(device)\n",
        "            tmp_loss = 0\n",
        "            for u,i,r in zip(u_list, i_list, r_list):\n",
        "                pred = model(u, i)\n",
        "                losses.append(loss_fn(torch.squeeze(pred), r.float()).item())\n",
        "            tmp_loss = np.mean(losses)\n",
        "            test_loss.append( np.sqrt(tmp_loss) )\n",
        "\n",
        "    test_loss_v  = np.mean(test_loss)\n",
        "    print(f\"Test Avg loss: {test_loss_v:>8f} \\n\")"
      ],
      "metadata": {
        "id": "H5WsBu33xX2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform the MF model"
      ],
      "metadata": {
        "id": "ih3ObcWQ25fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## DO: get the train & test iter tensors \n",
        "\n",
        "num_users, num_items, train_iter, test_iter = split_and_load_ml100k(\n",
        "    test_ratio=test_ratio, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "HrIoBv-U25Ni"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sBUl9bjkFi6q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, data in enumerate(train_iter):\n",
        "#     print(\"i: \", i)\n",
        "#     print('device:', device)\n",
        "#     u,i,r = data \n",
        "#     # print(\"data:\", data.to(device))\n",
        "#     print(\"u:\", u.to(device))\n",
        "#     print(\"i:\", i.to(device))\n",
        "#     print(\"r:\", r.to(device))\n",
        "#     break"
      ],
      "metadata": {
        "id": "xjLWs_drFixh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aGWBP8SYFiub"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fnpM3HU4FinD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## init the model, and feed the train & test dataset into GPU \n",
        "\n",
        "# train_iter.to(device)\n",
        "# test_iter.to(device)\n",
        "model = MF(30, num_users, num_items).to(device)\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "# define the optimizer \n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)"
      ],
      "metadata": {
        "id": "lIOX0GTh3KQs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_iter, model, loss_fn, optimizer)\n",
        "    test_loop(test_iter, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88hHm22Z2dEN",
        "outputId": "c43ad61a-7488-492d-9e0e-21bc4d397447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 6.70\n",
            "loss: 6.65\n",
            "loss: 6.61\n",
            "loss: 6.62\n",
            "loss: 6.60\n",
            "loss: 6.58\n",
            "loss: 6.57\n",
            "loss: 6.55\n",
            "loss: 6.54\n",
            "loss: 6.52\n",
            "loss: 6.51\n",
            "loss: 6.50\n",
            "loss: 6.48\n",
            "loss: 6.46\n",
            "loss: 6.45\n",
            "loss: 6.43\n",
            "loss: 6.42\n",
            "loss: 6.41\n",
            "loss: 6.40\n",
            "Test Avg loss: 5.880799 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 6.03\n",
            "loss: 6.01\n",
            "loss: 6.01\n",
            "loss: 5.99\n",
            "loss: 5.96\n",
            "loss: 5.95\n",
            "loss: 5.92\n",
            "loss: 5.90\n",
            "loss: 5.89\n",
            "loss: 5.87\n",
            "loss: 5.85\n",
            "loss: 5.84\n",
            "loss: 5.83\n",
            "loss: 5.81\n",
            "loss: 5.80\n",
            "loss: 5.78\n",
            "loss: 5.77\n",
            "loss: 5.76\n",
            "loss: 5.75\n",
            "Test Avg loss: 5.492755 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 5.32\n",
            "loss: 5.30\n",
            "loss: 5.34\n",
            "loss: 5.33\n",
            "loss: 5.31\n",
            "loss: 5.31\n",
            "loss: 5.32\n",
            "loss: 5.30\n",
            "loss: 5.29\n",
            "loss: 5.28\n",
            "loss: 5.26\n",
            "loss: 5.25\n",
            "loss: 5.24\n",
            "loss: 5.23\n",
            "loss: 5.22\n",
            "loss: 5.21\n",
            "loss: 5.21\n",
            "loss: 5.19\n",
            "loss: 5.18\n",
            "Test Avg loss: 5.146789 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 4.88\n",
            "loss: 4.90\n",
            "loss: 4.86\n",
            "loss: 4.84\n",
            "loss: 4.84\n",
            "loss: 4.84\n",
            "loss: 4.83\n",
            "loss: 4.81\n",
            "loss: 4.79\n",
            "loss: 4.78\n",
            "loss: 4.76\n",
            "loss: 4.75\n",
            "loss: 4.74\n",
            "loss: 4.74\n",
            "loss: 4.72\n",
            "loss: 4.71\n",
            "loss: 4.70\n",
            "loss: 4.70\n",
            "loss: 4.68\n",
            "Test Avg loss: 4.838115 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 4.40\n",
            "loss: 4.41\n",
            "loss: 4.41\n",
            "loss: 4.39\n",
            "loss: 4.39\n",
            "loss: 4.39\n",
            "loss: 4.37\n",
            "loss: 4.36\n",
            "loss: 4.35\n",
            "loss: 4.34\n",
            "loss: 4.34\n",
            "loss: 4.32\n",
            "loss: 4.31\n",
            "loss: 4.30\n",
            "loss: 4.29\n",
            "loss: 4.28\n",
            "loss: 4.27\n",
            "loss: 4.26\n",
            "loss: 4.25\n",
            "Test Avg loss: 4.560121 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 3.95\n",
            "loss: 3.94\n",
            "loss: 3.98\n",
            "loss: 3.98\n",
            "loss: 3.97\n",
            "loss: 3.96\n",
            "loss: 3.94\n",
            "loss: 3.94\n",
            "loss: 3.93\n",
            "loss: 3.93\n",
            "loss: 3.92\n",
            "loss: 3.91\n",
            "loss: 3.90\n",
            "loss: 3.88\n",
            "loss: 3.88\n",
            "loss: 3.87\n",
            "loss: 3.87\n",
            "loss: 3.86\n",
            "loss: 3.85\n",
            "Test Avg loss: 4.311023 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 3.57\n",
            "loss: 3.59\n",
            "loss: 3.57\n",
            "loss: 3.57\n",
            "loss: 3.56\n",
            "loss: 3.56\n",
            "loss: 3.55\n",
            "loss: 3.55\n",
            "loss: 3.55\n",
            "loss: 3.55\n",
            "loss: 3.55\n",
            "loss: 3.54\n",
            "loss: 3.54\n",
            "loss: 3.53\n",
            "loss: 3.53\n",
            "loss: 3.52\n",
            "loss: 3.52\n",
            "loss: 3.51\n",
            "loss: 3.50\n",
            "Test Avg loss: 4.086107 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 3.35\n",
            "loss: 3.29\n",
            "loss: 3.27\n",
            "loss: 3.26\n",
            "loss: 3.26\n",
            "loss: 3.26\n",
            "loss: 3.26\n",
            "loss: 3.25\n",
            "loss: 3.25\n",
            "loss: 3.25\n",
            "loss: 3.24\n",
            "loss: 3.24\n",
            "loss: 3.23\n",
            "loss: 3.22\n",
            "loss: 3.21\n",
            "loss: 3.21\n",
            "loss: 3.20\n",
            "loss: 3.20\n",
            "loss: 3.19\n",
            "Test Avg loss: 3.881924 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 3.02\n",
            "loss: 3.00\n",
            "loss: 3.00\n",
            "loss: 2.98\n",
            "loss: 2.97\n",
            "loss: 2.97\n",
            "loss: 2.96\n",
            "loss: 2.96\n",
            "loss: 2.95\n",
            "loss: 2.95\n",
            "loss: 2.94\n",
            "loss: 2.94\n",
            "loss: 2.94\n",
            "loss: 2.94\n",
            "loss: 2.93\n",
            "loss: 2.93\n",
            "loss: 2.92\n",
            "loss: 2.92\n",
            "loss: 2.91\n",
            "Test Avg loss: 3.698478 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.75\n",
            "loss: 2.75\n",
            "loss: 2.75\n",
            "loss: 2.74\n",
            "loss: 2.75\n",
            "loss: 2.74\n",
            "loss: 2.74\n",
            "loss: 2.72\n",
            "loss: 2.72\n",
            "loss: 2.71\n",
            "loss: 2.70\n",
            "loss: 2.70\n",
            "loss: 2.69\n",
            "loss: 2.69\n",
            "loss: 2.68\n",
            "loss: 2.68\n",
            "loss: 2.67\n",
            "loss: 2.67\n",
            "loss: 2.67\n",
            "Test Avg loss: 3.531982 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.48\n",
            "loss: 2.53\n",
            "loss: 2.52\n",
            "loss: 2.52\n",
            "loss: 2.51\n",
            "loss: 2.51\n",
            "loss: 2.50\n",
            "loss: 2.49\n",
            "loss: 2.49\n",
            "loss: 2.48\n",
            "loss: 2.48\n",
            "loss: 2.47\n",
            "loss: 2.47\n",
            "loss: 2.46\n",
            "loss: 2.46\n",
            "loss: 2.46\n",
            "loss: 2.46\n",
            "loss: 2.45\n",
            "loss: 2.45\n",
            "Test Avg loss: 3.382264 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.28\n",
            "loss: 2.29\n",
            "loss: 2.28\n",
            "loss: 2.27\n",
            "loss: 2.27\n",
            "loss: 2.28\n",
            "loss: 2.27\n",
            "loss: 2.27\n",
            "loss: 2.27\n",
            "loss: 2.27\n",
            "loss: 2.27\n",
            "loss: 2.26\n",
            "loss: 2.26\n",
            "loss: 2.26\n",
            "loss: 2.26\n",
            "loss: 2.26\n",
            "loss: 2.26\n",
            "loss: 2.25\n",
            "loss: 2.25\n",
            "Test Avg loss: 3.246570 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.11\n",
            "loss: 2.10\n",
            "loss: 2.10\n",
            "loss: 2.11\n",
            "loss: 2.11\n",
            "loss: 2.11\n",
            "loss: 2.10\n",
            "loss: 2.11\n",
            "loss: 2.10\n",
            "loss: 2.10\n",
            "loss: 2.10\n",
            "loss: 2.10\n",
            "loss: 2.09\n",
            "loss: 2.09\n",
            "loss: 2.09\n",
            "loss: 2.09\n",
            "loss: 2.08\n",
            "loss: 2.08\n",
            "loss: 2.08\n",
            "Test Avg loss: 3.122318 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.02\n",
            "loss: 1.98\n",
            "loss: 1.97\n",
            "loss: 1.96\n",
            "loss: 1.96\n",
            "loss: 1.96\n",
            "loss: 1.96\n",
            "loss: 1.96\n",
            "loss: 1.95\n",
            "loss: 1.95\n",
            "loss: 1.95\n",
            "loss: 1.94\n",
            "loss: 1.94\n",
            "loss: 1.94\n",
            "loss: 1.94\n",
            "loss: 1.93\n",
            "loss: 1.93\n",
            "loss: 1.93\n",
            "loss: 1.92\n",
            "Test Avg loss: 3.012047 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.81\n",
            "loss: 1.80\n",
            "loss: 1.80\n",
            "loss: 1.79\n",
            "loss: 1.80\n",
            "loss: 1.79\n",
            "loss: 1.79\n",
            "loss: 1.79\n",
            "loss: 1.79\n",
            "loss: 1.79\n",
            "loss: 1.79\n",
            "Test Avg loss: 2.910478 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.71\n",
            "loss: 1.70\n",
            "loss: 1.70\n",
            "loss: 1.70\n",
            "loss: 1.70\n",
            "loss: 1.70\n",
            "loss: 1.70\n",
            "loss: 1.69\n",
            "loss: 1.69\n",
            "loss: 1.69\n",
            "loss: 1.69\n",
            "loss: 1.68\n",
            "loss: 1.68\n",
            "loss: 1.68\n",
            "loss: 1.68\n",
            "loss: 1.67\n",
            "loss: 1.67\n",
            "loss: 1.67\n",
            "loss: 1.67\n",
            "Test Avg loss: 2.818550 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.60\n",
            "loss: 1.59\n",
            "loss: 1.59\n",
            "loss: 1.58\n",
            "loss: 1.58\n",
            "loss: 1.58\n",
            "loss: 1.58\n",
            "loss: 1.58\n",
            "loss: 1.58\n",
            "loss: 1.57\n",
            "loss: 1.57\n",
            "loss: 1.58\n",
            "loss: 1.57\n",
            "loss: 1.57\n",
            "loss: 1.57\n",
            "loss: 1.57\n",
            "loss: 1.57\n",
            "loss: 1.56\n",
            "loss: 1.56\n",
            "Test Avg loss: 2.735808 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.46\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.48\n",
            "loss: 1.48\n",
            "loss: 1.48\n",
            "loss: 1.48\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "loss: 1.47\n",
            "Test Avg loss: 2.659309 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.42\n",
            "loss: 1.42\n",
            "loss: 1.40\n",
            "loss: 1.40\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.40\n",
            "loss: 1.39\n",
            "loss: 1.40\n",
            "loss: 1.40\n",
            "loss: 1.40\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "loss: 1.39\n",
            "Test Avg loss: 2.589023 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.34\n",
            "loss: 1.32\n",
            "loss: 1.33\n",
            "loss: 1.33\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.31\n",
            "loss: 1.31\n",
            "loss: 1.32\n",
            "loss: 1.32\n",
            "loss: 1.31\n",
            "loss: 1.31\n",
            "Test Avg loss: 2.526005 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.27\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.26\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "loss: 1.25\n",
            "Test Avg loss: 2.468302 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.18\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.20\n",
            "loss: 1.20\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "loss: 1.19\n",
            "Test Avg loss: 2.413920 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.13\n",
            "loss: 1.13\n",
            "loss: 1.13\n",
            "loss: 1.13\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "loss: 1.14\n",
            "Test Avg loss: 2.364791 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.10\n",
            "loss: 1.11\n",
            "loss: 1.11\n",
            "loss: 1.11\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.09\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.10\n",
            "loss: 1.09\n",
            "Test Avg loss: 2.320063 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.07\n",
            "loss: 1.06\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n",
            "loss: 1.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Iu56fRgE6I3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}